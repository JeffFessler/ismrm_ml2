var documenterSearchIndex = {"docs":
[{"location":"generated/examples/03-flux-ring2/#03-flux-ring2","page":"03-flux-ring2","title":"03-flux-ring2","text":"","category":"section"},{"location":"generated/examples/03-flux-ring2/#Basic-Introduction-to-Machine-Learning:-03-flux-ring2","page":"03-flux-ring2","title":"Basic Introduction to Machine Learning: 03-flux-ring2","text":"","category":"section"},{"location":"generated/examples/03-flux-ring2/#Illustrate-basic-artificial-NN-training-using-Julia's-Flux-library","page":"03-flux-ring2","title":"Illustrate basic artificial NN training using Julia's Flux library","text":"Jeff Fessler, University of Michigan\n2018-10-18 Julia 1.0.1 original\n2023-01-29 Julia 1.8.5 update\n\nThis page was generated from a single Julia file: 03-flux-ring2.jl.\n\nIn any such Julia documentation, you can access the source code using the \"Edit on GitHub\" link in the top right.\n\nThe corresponding notebook can be viewed in nbviewer here: 03-flux-ring2.ipynb, and opened in binder here: 03-flux-ring2.ipynb.","category":"section"},{"location":"generated/examples/03-flux-ring2/#Setup","page":"03-flux-ring2","title":"Setup","text":"Packages needed here.\n\nusing LinearAlgebra: norm\nusing Random: seed!\nusing LaTeXStrings # pretty plot labels\nimport Flux # Julia package for deep learning\nusing Flux: Dense, Chain, relu, params, Adam, throttle, mse\nusing Plots: Plot, plot, plot!, scatter!, default, gui\nusing MIRTjim: jim, prompt\nusing InteractiveUtils: versioninfo\n\ndefault(markersize=5, markerstrokecolor=:auto, label=\"\")\ndefault(legendfontsize=10, labelfontsize=12, tickfontsize=10)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/examples/03-flux-ring2/#Generate-(synthetic)-data","page":"03-flux-ring2","title":"Generate (synthetic) data","text":"Function to simulate data that cannot be linearly separated\n\nfunction simdata(; n1 = 40, n2 = 120, σ1 = 0.8, σ2 = 2, r2 = 3)\n    data1 = σ1 * randn(2,n1)\n    rad2 = r2 .+ σ2*rand(1,n2)\n    ang2 = rand(1,n2) * 2π\n    data2 = [rad2 .* cos.(ang2); rad2 .* sin.(ang2)]\n    X = [data1 data2] # 2 × N = n1+n2\n    Y = [-ones(1,n1) ones(1,n2)] # 1 × N\n    @assert size(X,2) == size(Y,2)\n    return (X,Y)\nend;\nnothing #hide\n\nScatter plot routine\n\nfunction datasplit(X,Y)\n    data1 = X[:,findall(==(-1), vec(Y))]\n    data2 = X[:,findall(==(1), vec(Y))]\n    return (data1, data2)\nend;\n\nfunction plot_data(X,Y; kwargs...)\n    data1, data2 = datasplit(X,Y)\n    plot(xlabel=L\"x_1\", ylabel=L\"x_2\"; kwargs...)\n    scatter!(data1[1,:], data1[2,:], color=:blue, label=\"class1\")\n    scatter!(data2[1,:], data2[2,:], color=:red, label=\"class2\")\n    plot!(xlim=[-1,1]*6, ylim=[-1,1]*6)\n    plot!(aspect_ratio=1, xtick=-6:6:6, ytick=-6:6:6)\nend;\nnothing #hide\n\nTraining data\n\nseed!(0)\n(Xtrain, Ytrain) = simdata()\nplot_data(Xtrain,Ytrain)\n\nprompt()\n\nValidation and testing data\n\n(Xvalid, Yvalid) = simdata()\n(Xtest, Ytest) = simdata()\n\np1 = plot_data(Xvalid, Yvalid; title=\"Validation\")\np2 = plot_data(Xtest, Ytest; title=\"Test\")\nplot(p1, p2)\n\nprompt()","category":"section"},{"location":"generated/examples/03-flux-ring2/#Train-simple-MLP-model","page":"03-flux-ring2","title":"Train simple MLP model","text":"A [multilayer perceptron model] (https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) consists of multiple fully connected layers.\n\nTrain a basic NN model with 1 hidden layer\n\nif !@isdefined(state)\n    nhidden = 10 # neurons in hidden layer\n    model = Chain(Dense(2,nhidden,relu), Dense(nhidden,1))\n    loss3(model, x, y) = mse(model(x), y) # admittedly silly choice\n    iters = 10000\n    dataset = Base.Iterators.repeated((Xtrain, Ytrain), iters)\n    state = Flux.setup(Adam(), model)\n    Flux.train!(loss3, model, dataset, state)\nend;\nnothing #hide\n\nPlot results after training\n\nfunction display_decision_boundaries(\n    X, Y, model;\n    x1range = range(-1,1,101)*6, x2range = x1range, τ = 0.0,\n    kwargs...,\n)\n    data1,data2 = datasplit(X,Y)\n    D = [model([x1;x2])[1] for x1 in x1range, x2 in x2range]\n    jim(x1range, x2range, sign.(D.-τ); color=:grays, kwargs...)\n    scatter!(data1[1,:], data1[2,:], color = :blue, label = \"Class 1\")\n    scatter!(data2[1,:], data2[2,:], color = :red, label = \"Class 2\")\n    plot!(xlabel=L\"x_1\", ylabel=L\"x_2\")\n    plot!(xlim=[-1,1]*6, ylim=[-1,1]*6)\n    plot!(aspect_ratio=1, xtick=-6:6:6, ytick=-6:6:6)\nend;\nnothing #hide\n\nExamine classification accuracy\n\nclassacc(model, x, y::Number) = sign(model(x)[1]) == y\nclassacc(model, x, y::AbstractArray) = classacc(model, x, y[1])\nfunction classacc(X, Y)\n    tmp = zip(eachcol(X), eachcol(Y))\n    tmp = count(xy -> classacc(model, xy...), tmp)\n    tmp = tmp / size(Y,2) * 100\n    return round(tmp, digits=3)\nend\n\nlossXY = loss3(model, Xtrain, Ytrain)\ndisplay_decision_boundaries(Xtrain, Ytrain, model)\nplot!(title = \"Train: MSE Loss = $(round(lossXY,digits=4)), \" *\n    \"Class=$(classacc(Xtrain, Ytrain)) %\")\n\nprompt()","category":"section"},{"location":"generated/examples/03-flux-ring2/#Train-while-validating","page":"03-flux-ring2","title":"Train while validating","text":"Create a basic NN model with 1 hidden layer. This version evaluates performance every epoch for both the training data and validation data.\n\nnhidden = 10 # neurons in hidden layer\nlayer2 = Dense(2, nhidden, relu)\nlayer3 = Dense(nhidden, 1)\nmodel = Chain(layer2, layer3)\nloss3(model, x, y) = mse(model(x), y)\n\nnouter = 80 # of outer iterations, for showing loss\nlosstrain = zeros(nouter+1)\nlossvalid = zeros(nouter+1)\n\niters = 100\nlosstrain[1] = loss3(model, Xtrain, Ytrain)\nlossvalid[1] = loss3(model, Xvalid, Yvalid)\n\nfor io in 1:nouter\n    # @show io\n    idataset = Base.Iterators.repeated((Xtrain, Ytrain), iters)\n    istate = Flux.setup(Adam(), model)\n    Flux.train!(loss3, model, idataset, istate)\n    losstrain[io+1] = loss3(model, Xtrain, Ytrain)\n    lossvalid[io+1] = loss3(model, Xvalid, Yvalid)\n    if (io ≤ 6) && false # set to true to make images\n        display_decision_boundaries(Xtrain, Ytrain, model)\n        plot!(title=\"$(io*iters) epochs\")\n        # savefig(\"ml-flux-$(io*iters).pdf\")\n    end\nend\n\nloss_train = loss3(model, Xtrain, Ytrain)\nloss_valid = loss3(model, Xvalid, Yvalid)\np1 = display_decision_boundaries(Xtrain, Ytrain, model;\n title=\"Train:\\nMSE Loss = $(round(loss_train,digits=4))\\n\" *\n    \"Class=$(classacc(Xtrain, Ytrain)) %\",\n)\np2 = display_decision_boundaries(Xvalid, Yvalid, model;\n title=\"Valid:\\nMSE Loss = $(round(loss_valid,digits=4))\\n\" *\n    \"Class=$(classacc(Xvalid, Yvalid)) %\",\n)\np12 = plot(p1, p2)\n\nprompt()\n\nShow MSE loss vs epoch\n\nivalid = findfirst(>(0), diff(lossvalid))\nplot(xlabel=\"epoch/$(iters)\", ylabel=\"RMSE loss\", ylim=[0,1.05*maximum(losstrain)])\nplot!(0:nouter, sqrt.(losstrain), label=\"training loss\", marker=:o, color=:green)\nplot!(0:nouter, sqrt.(lossvalid), label=\"validation loss\", marker=:+, color=:violet)\nplot!(xticks = [0, ivalid, nouter])\n\nprompt()\n\nShow response of (trained) first hidden layer\n\nx1range = range(-1,1,31) * 6\nx2range = range(-1,1,33) * 6\nlayer2data = [layer2([x1;x2])[n] for x1 = x1range, x2 = x2range, n in 1:nhidden]\n\npl = Array{Plot}(undef, nhidden)\nfor n in 1:nhidden\n    ptmp = jim(x1range, x2range, layer2data[:,:,n], color=:cividis,\n        xtick=-6:6:6, ytick=-6:6:6,\n    )\n    if n == 7\n        plot!(ptmp, xlabel=L\"x_1\", ylabel=L\"x_2\")\n    end\n    pl[n] = ptmp\nend\nplot(pl[1:9]...)\n\nprompt()","category":"section"},{"location":"generated/examples/03-flux-ring2/#Reproducibility","page":"03-flux-ring2","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/examples/02-digits/#02-digits","page":"02-digits","title":"02-digits","text":"","category":"section"},{"location":"generated/examples/02-digits/#Basic-Introduction-to-Machine-Learning:-02-digits","page":"02-digits","title":"Basic Introduction to Machine Learning: 02-digits","text":"","category":"section"},{"location":"generated/examples/02-digits/#Hand-written-digit-classification","page":"02-digits","title":"Hand-written digit classification","text":"2018-10-23 Jeff Fessler, University of Michigan\n\nThis page was generated from a single Julia file: 02-digits.jl.\n\nIn any such Julia documentation, you can access the source code using the \"Edit on GitHub\" link in the top right.\n\nThe corresponding notebook can be viewed in nbviewer here: 02-digits.ipynb, and opened in binder here: 02-digits.ipynb.","category":"section"},{"location":"generated/examples/02-digits/#Setup","page":"02-digits","title":"Setup","text":"Packages needed here.\n\nusing LinearAlgebra: norm, svd\nusing StatsBase: mean\nusing MLDatasets: MNIST\nusing Random: seed!, randperm\nusing LaTeXStrings # pretty plot labels\nusing Plots: default, gui, plot, savefig, scatter, plot!, scatter!\nusing MIRTjim: jim, prompt\nusing InteractiveUtils: versioninfo\ndefault(markersize=5, markerstrokecolor=:auto, label=\"\")\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/examples/02-digits/#Load-data","page":"02-digits","title":"Load data","text":"Read the MNIST data for some handwritten digits. This code will automatically download the data from web if needed and put it in a folder like: ~/.julia/datadeps/MNIST/.\n\nif !@isdefined(data)\n    digitn = (4, 9) # which digits to use\n    isinteractive() || (ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true) # avoid prompt\n    dataset = MNIST(Float32, :train)\n    nrep = 1000\n    # function to extract the 1st 1000 examples of digit n:\n    data = n -> dataset.features[:,:,findall(==(n), dataset.targets)[1:nrep]]\n    data = 255 * cat(dims=4, data.(digitn)...)\n    nx, ny, nrep, ndigit = size(data)\n    data = data[:,2:ny,:,:] # make images non-square to force debug\n    ny = size(data,2)\n    d0 = data[:,:,:,1]\n    d1 = data[:,:,:,2]\n    size(data)\nend\n\nLook at sorted and unsorted images to show (un)supervised\n\nseed!(0)\nnrow = 4\nncol = 6\nt0 = d0[:,:,1:nrow*ncol÷2]\nt0[:,:,6] = d0[:,:,222] # include one ambiguous case\nt1 = d1[:,:,1:nrow*ncol÷2]\ntmp = cat(t0, t1, dims=3)\njim(tmp)\n# savefig(\"02-digit-sort.pdf\")\n\ntmp = tmp[:,:,randperm(size(tmp,3))] # for unsupervised\npu = jim(tmp; xticks=false, yticks=false, colorbar=:none); # book\n# savefig(\"02-digit-rand.pdf\")\n# savefig(pu, \"data-4-9.pdf\")\n\nUse some data for training, and some for test\n\nntrain = 100\nntest = nrep - ntrain\ntrain0 = d0[:,:,1:ntrain] # training data\ntrain1 = d1[:,:,1:ntrain]\ntest0 = d0[:,:,(ntrain+1):end] # testing data\ntest1 = d1[:,:,(ntrain+1):end];\nnothing #hide\n\nSVD for singular vectors and low-rank subspace approximation\n\nu0 = svd(reshape(train0, nx*ny, :)).U\nu1 = svd(reshape(train1, nx*ny, :)).U\nr0 = 3 # selected ranks\nr1 = 3\nq0 = reshape(u0[:,1:r0], nx, ny, :)\nq1 = reshape(u1[:,1:r1], nx, ny, :)\np0 = jim(q0; nrow = 1)\np1 = jim(q1; nrow = 1)\np01 = plot(p0, p1, layout=(2,1))\n\nprompt()","category":"section"},{"location":"generated/examples/02-digits/#How-well-do-the-first-left-singular-vectors-separate-the-two-classes?","page":"02-digits","title":"How well do the first left singular vectors separate the two classes?","text":"regress = (data, u) -> vec(mapslices(slice -> u'*slice[:], data, dims=(1,2)))\nl1 = \"$(digitn[1])\"\nl2 = \"$(digitn[2])\"\nplot(xlabel = l1 * \" U[:,1]\", ylabel = l2 * \" U[:,1]\", legend=:topleft)\nscatter!(regress(train0, u0[:,1]), regress(train0, u1[:,1]), label=l1)\nscatter!(regress(train1, u0[:,1]), regress(train1, u1[:,1]), label=l2)\n\nprompt()","category":"section"},{"location":"generated/examples/02-digits/#Classify-test-digits-based-on-nearest-subspace","page":"02-digits","title":"Classify test digits based on nearest subspace","text":"Q0 = reshape(q0, nx*ny, r0)\nQ1 = reshape(q1, nx*ny, r1);\n\ny0 = reshape(test0, nx*ny, :)\ny00 = y0 - Q0 * (Q0' * y0)\ny01 = y0 - Q1 * (Q1' * y0)\ncorrect0 = (mapslices(norm, y00, dims=1) .< mapslices(norm, y01, dims=1))\nc0 = count(correct0) / ntest\n\ny1 = reshape(test1, nx*ny, :)\ny10 = y1 - Q0 * (Q0' * y1)\ny11 = y1 - Q1 * (Q1' * y1)\ncorrect1 = (mapslices(norm, y10, dims=1) .> mapslices(norm, y11, dims=1))\nc1 = count(correct1) / ntest","category":"section"},{"location":"generated/examples/02-digits/#If-I-had-more-time-I-would-show-CNN-based-classification-here...","page":"02-digits","title":"If I had more time I would show CNN-based classification here...","text":"","category":"section"},{"location":"generated/examples/02-digits/#Reproducibility","page":"02-digits","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/examples/04-denoise-1d/#04-denoise-1d","page":"04-denoise-1d","title":"04-denoise-1d","text":"","category":"section"},{"location":"generated/examples/04-denoise-1d/#Basic-Introduction-to-Machine-Learning:-04-denoise-1d","page":"04-denoise-1d","title":"Basic Introduction to Machine Learning: 04-denoise-1d","text":"Illustrate 1D signal denoising using Julia's Flux library\n\nJeff Fessler, University of Michigan\n2018-10-23 Julia 1.0.1 version\n2023-01-29 Julia 1.8.5 version\n\nThis page was generated from a single Julia file: 04-denoise-1d.jl.\n\nIn any such Julia documentation, you can access the source code using the \"Edit on GitHub\" link in the top right.\n\nThe corresponding notebook can be viewed in nbviewer here: 04-denoise-1d.ipynb, and opened in binder here: 04-denoise-1d.ipynb.","category":"section"},{"location":"generated/examples/04-denoise-1d/#Setup","page":"04-denoise-1d","title":"Setup","text":"Packages needed here.\n\nusing LinearAlgebra: norm, I\nusing Random: seed!\nusing Distributions: Normal, randperm\nimport Flux # Julia package for deep learning\nusing Flux: Dense, Conv, Chain, SkipConnection, Adam, mse, relu, SamePad\nusing LaTeXStrings # pretty plot labels\nusing Plots: plot, plot!, scatter, scatter!, histogram, histogram2d, default, font, gui\nusing MIRTjim: jim, prompt\nusing InteractiveUtils: versioninfo\n\ndefault(markersize=5, markerstrokecolor=:auto, label=\"\")\ndefault(tickfontsize=10, legendfontsize=11)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/examples/04-denoise-1d/#Training-data","page":"04-denoise-1d","title":"Training data","text":"Generate training and testing data; 1D piece-wise constant signals\n\nFunction to generate a random piece-wise constant signal\n\nfunction makestep(; dim=32, njump=3, valueDist=Normal(0, 1), minsep=2)\n    jump_locations = randperm(dim)[1:njump]\n    while minimum(diff(jump_locations)) <= minsep\n        jump_locations = randperm(dim)[1:njump] # random jump locations\n    end\n    index = zeros(dim)\n    index[jump_locations] .= 1\n    index = cumsum(index)\n    values = rand(valueDist, njump) # random signal values\n    x = zeros(Float32, dim)\n    for jj in 1:njump\n        x[index .== jj] .= values[jj]\n    end\n    x[index .== 0] .= values[njump] # periodic end conditions\n    x = circshift(x, rand(1:dim, 1)) # random shift - just to be sure\n    return x\nend\n\nTraining data\n\nseed!(0)\nsiz = 32\n\nntrain = 2^11\nXtrain = [makestep(dim=siz) for _ in 1:ntrain] # noiseless data\nXtrain = hcat(Xtrain...) # (siz, ntrain)\n\nntest = 2^10\nXtest = [makestep(dim=siz) for _ in 1:ntest] # noiseless data\nXtest = hcat(Xtest...) # (siz, ntest)\n\np0 = plot(Xtrain[:,1:14], label=\"\")\n\nprompt()\n\nData covariance\n\nKx = Xtrain * Xtrain' / ntrain\np1 = jim(Kx, title=\"Kx\", color=:cividis);\nnothing #hide\n\nAdd noise\n\nσnoise = 0.3\nYtrain = Xtrain + σnoise * randn(Float32, size(Xtrain)) # noisy train data\nYtest = Xtest + σnoise * randn(Float32, size(Xtest)) # noisy test data\nKy = Ytrain * Ytrain' / ntrain;\n\n@show maximum(Kx)\n@show maximum(Ky)\n\np2 = jim(Ky, title=\"Ky\", color=:cividis)\nplot(p0, p1, p2, layout=(3,1))\n\nprompt()\n\nWiener filter (MMSE estimator)\n\n# cond(Kx + σnoise*I)\nHw = Kx * inv(Kx + σnoise^2 * I)\njim(Hw; title=\"Wiener filter\", color=:cividis)\n\nDenoise via Wiener filter (MMSE linear method)\n\nXw = Hw * Ytest\nnrmse = (Xh) -> round(norm(Xh - Xtest) / norm(Xtest) * 100, digits=2)\n@show nrmse(Ytest), nrmse(Xw)\ncolors = [:blue, :red, :magenta, :green]\nplot(ylabel=\"signal value\", title=\"Wiener filtering examples\")\nfor i in 1:4\n    plot!(Xw[:,i], color=colors[i])\n    plot!(Xtest[:,i], color=colors[i], line=:dash)\n    scatter!(Ytest[:,i], color=colors[i], marker=:star)\nend\n\nprompt()\n\nVerify that marginal distribution is Gaussian\n\nhistogram(Xtrain[:], label = \"Xtrain hist\")\n\nprompt()","category":"section"},{"location":"generated/examples/04-denoise-1d/#Simple-NN","page":"04-denoise-1d","title":"Simple NN","text":"Examine a \"NN\" that is a single fully connected affine layer (It should perform same as Wiener filter.)\n\nFirst try a basic affine NN model\n\nif !@isdefined(state1)\n    model1 = Chain(Dense(siz,siz))\n    loss3(model, x, y) = mse(model(x), y)\n\n    iters = 2^12\n    dataset = Base.Iterators.repeated((Ytrain, Xtrain), iters) # trick X,Y swap for denoising!\n\n    state1 = Flux.setup(Adam(), model1)\n    Flux.train!(loss3, model1, dataset, state1)\nend;\nnothing #hide\n\nCompare training affine NN to wiener filter\n\nH1 = model1(Matrix(I, siz, siz))\njim(H1; title=\"Learned filter for affine NN\", colormap=:cividis)\n\nDenoise test Data\n\nX1 = H1 * Ytest\nX1nn = model1(Ytest)\n@show nrmse(Ytest), nrmse(Xw), nrmse(X1), nrmse(X1nn)\nbias = model1(zeros(siz))\n@show extrema(bias)","category":"section"},{"location":"generated/examples/04-denoise-1d/#Examine-a-single-hidden-layer-NN","page":"04-denoise-1d","title":"Examine a single hidden layer NN","text":"","category":"section"},{"location":"generated/examples/04-denoise-1d/#Create-a-basic-NN-model","page":"04-denoise-1d","title":"Create a basic NN model","text":"if !@isdefined(state2)\n    nhidden = 2siz # neurons in hidden layer\n    model2 = Chain(Dense(siz, nhidden, relu), Dense(nhidden, siz))\n\n    iters = 2^12\n    dataset = Base.Iterators.repeated((Ytrain, Xtrain), iters) # trick X,Y swap for denoising!\n\n    state2 = Flux.setup(Adam(), model2)\n    Flux.train!(loss3, model2, dataset, state2)\n    X2 = model2(Ytest)\nend\ntmp = [Ytest, Xw, X1, X1nn, X2]\n@show nrmse.(tmp)\n\nExamine joint distribution\n\nlag = 1\ntmp = circshift(Xtrain, (lag,))\nhistogram2d(vec(Xtrain), vec(tmp))\nplot!(aspect_ratio=1, xlim=[-4,4], ylim=[-4,4])\nplot!(xlabel=L\"x[n]\", ylabel=latexstring(\"x[n-$lag \\\\ mod \\\\ N]\"))\nplot!(title=\"Joint histogram of neighbors\")\n\nprompt()","category":"section"},{"location":"generated/examples/04-denoise-1d/#WIP","page":"04-denoise-1d","title":"WIP","text":"Experiments below here - work in progress [https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl]\n\nif !@isdefined(model3)\n\n    model3 = SkipConnection( # ResNet style: learn residual\n        Chain(\n            Conv((3,), 1 => 16, relu; pad = SamePad()),\n        #  x -> maxpool(x, (2,2)),\n            Conv((3,), 16 => 8, relu; pad = SamePad()),\n            Conv((1,), 8 => 1, relu),\n        #  x -> reshape(x, :, size(x, 4)),\n        ),\n        +,\n    )\n\n    shaper(X) = reshape(X, siz, 1, :) # (siz, channels, batch)\n    mymodel3(X) = model3(shaper(X))[:, 1, :]\n    @assert size(mymodel3(Xtrain)) == size(Xtrain)\n\n    nouter = 2^2\n    ninner = 2^3\n    # trick X,Y swap for denoising!\n    dataset = Base.Iterators.repeated((shaper(Ytrain), shaper(Xtrain)), ninner)\n    for io in 1:nouter\n        state3 = Flux.setup(Adam(), model3)\n        Flux.train!(loss3, model3, dataset, state3)\n        X3train = mymodel3(Ytrain)\n        @show io, loss3(model3, shaper(Xtrain), shaper(Ytrain))\n        # todo: validation data too\n    end\nend\n\n\nX3test = mymodel3(Ytest)\ntmp = [Ytest, Xw, X1, X1nn, X2, X3test]\n@show nrmse.(tmp) # todo: no improvement!?","category":"section"},{"location":"generated/examples/04-denoise-1d/#Reproducibility","page":"04-denoise-1d","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/examples/01-overview/#01-overview","page":"01-overview","title":"01-overview","text":"","category":"section"},{"location":"generated/examples/01-overview/#Basic-Introduction-to-Machine-Learning:-01-overview","page":"01-overview","title":"Basic Introduction to Machine Learning: 01-overview","text":"This page was generated from a single Julia file: 01-overview.jl.\n\nIn any such Julia documentation, you can access the source code using the \"Edit on GitHub\" link in the top right.\n\nThe corresponding notebook can be viewed in nbviewer here: 01-overview.ipynb, and opened in binder here: 01-overview.ipynb.","category":"section"},{"location":"generated/examples/01-overview/#Setup","page":"01-overview","title":"Setup","text":"Packages needed here.\n\nusing LinearAlgebra: norm\nusing Random: seed!\nusing LaTeXStrings # pretty plot labels\nusing Plots: plot, plot!, scatter, scatter!, surface!, default, font, gui\nusing MIRTjim: jim, prompt\nusing InteractiveUtils: versioninfo\n\ndefault(markersize=5, markerstrokecolor=:auto, label=\"\")\nfnt = font(\"DejaVu Sans\", 15) # larger default font\ndefault(guidefont=fnt, xtickfont=fnt, ytickfont=fnt, legendfont=fnt)\ndefault(tickfontsize=10, legendfontsize=11)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/examples/01-overview/#Supervised-learning:-classification","page":"01-overview","title":"Supervised learning: classification","text":"seed!(0)\nn1 = 50; n2 = n1\nrot = phi -> [cos(phi) sin(-phi); sin(phi) cos(phi)]\ndata1 = rot(π/8) * ([3 0; 0 1] * randn(2,n1) .+ [8;2])\ndata2 = rot(π/4) * ([2 0; 0 1] * randn(2,n2) .+ [9;3])\nscatter(data1[1,:], data1[2,:], color=:blue, label=\"class1\")\nscatter!(data2[1,:], data2[2,:], color=:red, label=\"class2\")\nplot!(xlabel=L\"x_1\", ylabel=L\"x_2\")\nplot!(xlim=(0,14), ylim=(0,14))\nplot!(aspect_ratio=1, xtick=[0, 14], ytick=[0, 14])\nx = LinRange(0,14,101)\ny = 2 .+ x - 0.03 * x.^2 # add decision boundary\nplot!(x, y, color=:magenta)\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Supervised-learning:-regression","page":"01-overview","title":"Supervised learning: regression","text":"seed!(0)\nN = 40\nf = (x) -> 10. / (x + 1)\nxt = 10 * rand(N)\nyt = f.(xt) + 0.4 * randn(N)\nx = range(0, 10, 101)\ny = f.(x)\nscatter(xt, yt, color=:blue, label=\"training data\")\nplot!(xlabel=L\"x\", ylabel=L\"y\")\nplot!(xlim=(0,10), ylim=(0,8))\nplot!(xtick=0:5:10, ytick=0:4:8)\n\nPolynomial regression model\n\nAfun = (tt) -> [t.^i for t in tt, i in 0:3] # matrix of monomials\nA = Afun(xt)\ncoef = A \\ yt\ny = Afun(x) * coef\nplot!(x, y, line=:magenta, label=\"cubic regression\")\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Unsupervised-learning:-data","page":"01-overview","title":"Unsupervised learning: data","text":"seed!(0)\nn1 = 50; n2 = n1; n3=n1\nrot = phi -> [cos(phi) sin(-phi); sin(phi) cos(phi)]\ndata1 = rot(π/4) * ([2 0; 0 0.7] * randn(2,n1) .+ [9;3])\ndata2 = rot(π/8) * ([3 0; 0 0.6] * randn(2,n2) .+ [8;2])\ndata3 = rot( 0 ) * ([2 0; 0 0.5] * randn(2,n3) .+ [9;1]);\n\nplot(xlabel = L\"x_1\", ylabel = L\"x_2\")\nscatter!(data1[1,:], data1[2,:], color=:black, label=\"training data\")\nscatter!(data2[1,:], data2[2,:], color=:black)\nscatter!(data3[1,:], data3[2,:], color=:black)\nplot!(xlim=(0,14), ylim=(0,14))\nplot!(aspect_ratio=1, xtick=[0, 14], ytick=[0, 14])\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Clustering-(oracle)","page":"01-overview","title":"Clustering (oracle)","text":"plot(xlabel = L\"x_1\", ylabel = L\"x_2\")\nscatter!(data1[1,:], data1[2,:], color=:blue, label=\"cluster1\")\nscatter!(data2[1,:], data2[2,:], color=:red, label=\"cluster2\")\nscatter!(data3[1,:], data3[2,:], color=:orange, label=\"cluster3\")\nplot!(xlim=(0,14), ylim=(0,14))\nplot!(aspect_ratio=1, xtick=[0, 14], ytick=[0, 14])\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Novelty-detection","page":"01-overview","title":"Novelty detection","text":"plot(xlabel=L\"x_1\", ylabel=L\"x_2\")\nscatter!(data1[1,:], data1[2,:], color=:black)\nscatter!(data2[1,:], data2[2,:], color=:black)\nscatter!(data3[1,:], data3[2,:], color=:black)\nscatter!([10], [11], color=:red)\nplot!(xlim=(0,14), ylim=(0,14))\nplot!(aspect_ratio=1, xtick=[0, 14], ytick=[0, 14])\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#The-utility-of-nonlinearity","page":"01-overview","title":"The utility of nonlinearity","text":"1D plot supervised learning: classification\n\nseed!(0)\nn1 = 20; n2 = n1; n3 = n1\ndata1 = 1 * randn(2,n1) .+ 5\ndata2 = 1 * randn(2,n2) .+ 0\ndata3 = 1 * randn(2,n3) .+ (-5)\nplot(xlabel=L\"x_1\", ylabel=\"\")\nscatter!(data1[1,:], zeros(n1), color=:blue, label=\"class1\")\nscatter!(data2[1,:], zeros(n2), color=:red, label=\"class2\")\nscatter!(data3[1,:], zeros(n3), color=:blue)\nplot!(xlim=(-8,7), ylim=(-1,1))\nplot!(xtick=-6:3:6, ytick=[])\nplot!([1, 1]*2, [-1, 1], color=:orange)\n\nprompt()\n\nA simple nonlinearity, abs(feature), allows linear separation\n\nf = x -> abs(x)\ndata1[2,:] = f.(data1[1,:])\ndata2[2,:] = f.(data2[1,:])\ndata3[2,:] = f.(data3[1,:])\nplot(xlabel=L\"x_1\", ylabel=L\"x_2\")\nscatter!(data1[1,:], data1[2,:], color=:blue, label=\"class1\")\nscatter!(data2[1,:], data2[2,:], color=:red, label=\"class2\")\nscatter!(data3[1,:], data3[2,:], color=:blue)\nplot!(xlim=(-8,7), ylim=(-1,10))\nplot!(xtick=-6:3:6, ytick=0:5:10)\nplot!([-1, 1]*8, [1, 1]*2.4, color=:orange, width=2, legend=:top)\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#2D-example","page":"01-overview","title":"2D example","text":"seed!(0)\nn1 = 40; n2 = 120\ndata1 = randn(2,n1)\nrad2 = 3 .+ 3*rand(1,n2)\nang2 = rand(1,n2) * 2π\ndata2 = [rad2 .* cos.(ang2); rad2 .* sin.(ang2)]\nplot(xlabel=L\"x_1\", ylabel=L\"x_2\")\nscatter!(data1[1,:], data1[2,:], color=:blue, label=\"class1\")\nscatter!(data2[1,:], data2[2,:], color=:red, label=\"class2\")\nplot!(xlim=[-1,1]*6, ylim=[-1,1]*6)\nplot!(aspect_ratio=1, xtick=-6:6:6, ytick=-6:6:6)\n\nprompt()\n\nplot!([0, 1, 0, -1, 0]*3, [-1, 0, 1, 0, -1]*3, color=:orange, width=2)\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Nonlinear-lifting-into-3D","page":"01-overview","title":"Nonlinear lifting into 3D","text":"lift_fun = (x) -> sum(abs.(x), dims=1)\nlift1 = [data1; lift_fun(data1)]\nlift2 = [data2; lift_fun(data2)]\nplot(xlabel=L\"x_1\", ylabel=L\"x_2\", zlabel=L\"$x_3 = |x_1| + |x_2|$\")\nscatter!(lift1[1,:], lift1[2,:], lift1[3,:], color=:blue, label=\"class1\")\nscatter!(lift2[1,:], lift2[2,:], lift2[3,:], color=:red, label=\"class2\")\nplot!(xlim=[-1,1]*6, ylim=[-1,1]*6)\nplot!(xtick=-6:6:6, ytick=-6:6:6)\nplot!(camera=(30,12))\n#savefig(\"ml-nonlin2d-lift.pdf\")\n\nprompt()\n\n\nxc = -6:6\nyc = -6:6\nz = 3 * ones(length(xc), length(yc)) # 3 chosen manually\nsurface!(xc, yc, z, colorbar=nothing, alpha=0.6) #, color=:orange)\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Nonlinearity-in-regression","page":"01-overview","title":"Nonlinearity in regression","text":"seed!(0)\nN = 40\nf = (x) -> 10. / (x + 1)\nxt = 10 * rand(N)\nyt = f.(xt) + 0.4 * randn(N)\nx = range(0, 10, 101)\ny = f.(x)\nscatter(xt, yt, color=:blue, label=\"training data for regression\")\nplot!(xlabel=L\"x\", ylabel=L\"y\")\nplot!(xlim=(0,10), ylim=(0,8))\nplot!(xtick=0:5:10, ytick=0:4:8)\n\nAfun = (tt,deg) -> [t.^i for t in tt, i in 0:deg] # matrix of monomials\nA3 = Afun(xt,3)\ncoef3 = A3 \\ yt\ny3 = Afun(x,3) * coef3;\n\nA1 = Afun(xt,1)\ncoef1 = A1 \\ yt\ny1 = Afun(x,1) * coef1;\n\nplot!(x, y3, line=:magenta,\n    label = L\"\\mathrm{cubic:\\ } y = \\alpha_3 x^3 + \\alpha_2 x^2 + \\alpha_1 x + \\alpha_0\")\nplot!(x, y1, line=(:dash,:red),\n    label = L\"\\mathrm{linear\\ (affine):\\ } y = \\alpha_1 x + \\alpha_0\")\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Linear-discriminant-analysis-(LDA)","page":"01-overview","title":"Linear discriminant analysis (LDA)","text":"LDA\n\nseed!(0)\nn1 = 70; n2 = n1\nrot = phi -> [cos(phi) sin(-phi); sin(phi) cos(phi)]\nmu1 = [7, 10]\nmu2 = [9, 4]\nS1 = rot(π/9) * [3 0; 0 1]\nS2 = S1 # for LDA\ndata1 = S1 * randn(2,n1) .+ mu1\ndata2 = S2 * randn(2,n2) .+ mu2\nplot(xlabel=L\"x_1\", ylabel=L\"x_2\")\nscatter!(data1[1,:], data1[2,:], color=:blue, label=\"class1\")\nscatter!(data2[1,:], data2[2,:], color=:red, label=\"class2\")\nplot!(xlim=(0,16), ylim=(0,16))\nplot!(aspect_ratio=1)\nplot!(xtick=0:4:16, ytick=0:4:16)\n\nϕ = range(0,2π,101)\nfor r in [1.5 2.5]\n    local x = r * cos.(ϕ)\n    local y = r * sin.(ϕ)\n    c1 = S1 * [x'; y'] .+ mu1\n    c2 = S2 * [x'; y'] .+ mu2\n    plot!(c1[1,:], c1[2,:], color=:blue)\n    plot!(c2[1,:], c2[2,:], color=:red)\nend\nx = range(-1,17,11)\nw = (S1 * S1') \\ (mu2 - mu1) # LDA\nc = (norm(S1 \\ mu2)^2 - norm(S1 \\ mu1)^2)/2\ny = (c .- w[1] * x) / (w[2])\nplot!(x, y, color=:magenta, width=2, legend=:topleft)\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Model-order-selection","page":"01-overview","title":"Model-order selection","text":"Sinusoidal regression training data\n\nseed!(0)\nNtrain = 40\nNtest = 30\nf = (x) -> 10. / (x + 1)\nxtrain = 10 * rand(Ntrain)\nytrain = f.(xtrain) + 0.4 * randn(Ntrain)\nxtest = 10 * rand(Ntest)\nytest = f.(xtest) + 0.4 * randn(Ntest)\n\nx = range(0,10,201)\ny = f.(x)\n\nplot(xlabel=L\"x\", ylabel=L\"y\")\nscatter!(xtrain, ytrain, color=:blue, label=\"training data\")\nscatter!(xtest, ytest, color=:red, label=\"test data\")\nplot!(xlim=(0,10), ylim=(0,8))\nplot!(xtick=0:5:10, ytick=0:4:8)\n\nprompt()\n\nShow overfit\n\nscatter(xtrain, ytrain, color=:blue, label=\"training data\")\nplot!(xlim=(0,10), ylim=(0,8))\nplot!(xtick=0:5:10, ytick=0:4:8)\n\nAfun = (tt,deg) -> [t.^i for t in tt, i in 0:deg] # matrix of monomials\nAfun = (tt,deg) -> [cos(2π*t*i/20) for t in tt, i in 0:deg] # matrix of sinusoids\ndlist = [2 9 20]\nclist = (:magenta, :red, :orange)\nfor ii in 1:length(dlist)\n    local deg = dlist[ii]\n    local A = Afun(xtrain,deg)\n    local coef = A \\ ytrain\n    local y = Afun(x,deg) * coef\n    plot!(x, y, line=clist[ii], width=2, label=\"$deg harmonics\")\nend\nplot!(xlabel=L\"x\", ylabel=L\"y\")\n\nprompt()\n\nFit improves with more harmonics, of course\n\ndlist = 0:30\netrain = zeros(length(dlist))\netest = zeros(length(dlist))\nerrs = zeros(length(dlist))\nfor ii in 1:length(dlist)\n    deg = dlist[ii]\n    Atrain = Afun(xtrain, deg)\n    Atest = Afun(xtest, deg)\n    # @show cond(A'*A) # sinusoids is more stable than polynomials\n    local coef = Atrain \\ ytrain\n    yh = Atrain * coef\n    etrain[ii] = norm(yh - ytrain)\n    etest[ii] = norm(Atest * coef - ytest)\n    errs[ii] = norm(yh - f.(xt))\nend\nscatter(dlist, etrain, color=:blue, label=\"fit to training data\")\nplot!(xlabel = \"model order: # of sinusoids\")\nplot!(ylabel = L\"\\mathrm{fit:\\ } ‖ \\hat{y} - y ‖_2\")\nplot!(ylim=[0,13], ytick=[0,13])\nscatter!(dlist, etest, color=:red, label=\"fit to test data\")\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Cross-validation","page":"01-overview","title":"Cross-validation","text":"Nlearn = Int(Ntrain / 2)\nNvalid = Ntrain - Nlearn\nxlearn = xtrain[1:Nlearn]\nylearn = ytrain[1:Nlearn]\nxvalid = xtrain[(Nlearn+1):Ntrain]\nyvalid = ytrain[(Nlearn+1):Ntrain]\n\nplot(xlabel=L\"x\", ylabel=L\"y\")\nscatter!(xlearn, ylearn, color=:blue, label=\"training data (fitting)\")\nscatter!(xvalid, yvalid, color=:cyan, label=\"validation data (model selection)\")\nplot!(xlim=(0,10), ylim=(0,8))\nplot!(xtick=0:5:10, ytick=0:4:8)\n\nprompt()\n\nfit improves with more harmonics, of course\n\ndlist = 0:20\nelearn = zeros(length(dlist))\nevalid = zeros(length(dlist))\netest = zeros(length(dlist))\nerrs = zeros(length(dlist))\nfor ii in 1:length(dlist)\n    deg = dlist[ii]\n    Alearn = Afun(xlearn, deg)\n    Avalid = Afun(xvalid, deg)\n    Atest = Afun(xtest, deg)\n    # @show cond(A'*A) # sinusoids is more stable than polynomials\n    local coef = Alearn \\ ylearn\n    elearn[ii] = norm(Alearn * coef - ylearn)\n    evalid[ii] = norm(Avalid * coef - yvalid)\n    etest[ii] = norm(Atest * coef - ytest)\n    errs[ii] = norm([Alearn; Avalid]*coef - f.([xlearn; xvalid]))\nend\nscatter(dlist, elearn, color=:blue, label=\"fit to training data\")\nscatter!(dlist, evalid, color=:cyan, label=\"fit to validation data\")\nplot!(xlabel = \"model order: # of sinusoids\")\nplot!(ylabel = L\"fit: \\ ‖ \\hat{y} - y ‖_2\")\nplot!(ylim=[0,13], ytick=[0,13])\ndbest = findall(diff(evalid) .>= 0)[1] # find first increase in validation error\nplot!(xtick=[0, dlist[dbest], 20])\nscatter!(dlist, etest, color=:red, label=\"fit to test data\")\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Reproducibility","page":"01-overview","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#Machine-learning-tutorials","page":"Home","title":"Machine learning tutorials","text":"[https://github.com/JeffFessler/ismrmml2] (https://github.com/JeffFessler/ismrmml2)\n\nOriginally this work was located at [https://web.eecs.umich.edu/~fessler/papers/files/talk/18/ml2jf] (https://web.eecs.umich.edu/~fessler/papers/files/talk/18/ml2jf)\n\nSee the \"Examples\".","category":"section"}]
}
